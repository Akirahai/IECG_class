{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "so3g52ujQdBe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.io import ImageReadMode\n",
        "from torchvision.io import read_image\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch \n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torchvision import transforms as T\n",
        "import cv2\n",
        "import math\n",
        "from sklearn.metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zqFgIV1WQjEZ",
        "outputId": "afc59bcd-cb1a-4865-d505-e0b92fec5bfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda:0'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FHuHETCRDmP",
        "outputId": "8365e45b-8f2f-41ae-9480-2ee1eca3c8cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Recording  Label\n",
            "0        A4363      6\n",
            "1        A4447      4\n",
            "2        A0822      5\n",
            "3        A0415      2\n",
            "4        A5712      1\n",
            "...        ...    ...\n",
            "6184     A3728      5\n",
            "6185     A0683      1\n",
            "6186     A6561      5\n",
            "6187     A5674      2\n",
            "6188     A3270      3\n",
            "\n",
            "[6189 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('train_image.csv')\n",
        "test = pd.read_csv('val_image.csv')\n",
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1ZC5kM9NRIus"
      },
      "outputs": [],
      "source": [
        "class ECGimage_DB(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] +'.png')\n",
        "        image = read_image(img_path, mode=ImageReadMode.RGB)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image.float(), label-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XdeywH3ff41D"
      },
      "outputs": [],
      "source": [
        "class ECGimage_DBv(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] +'.png')\n",
        "        image = read_image(img_path, mode=ImageReadMode.RGB)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image.float(), label-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ctMa4oXaRftA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lehai\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\lehai\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 9)\n",
        "model = model.to(device)\n",
        "learning_rate = 1e-3\n",
        "epochs = 5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "REkWHXoeTNqX"
      },
      "outputs": [],
      "source": [
        "path=\"ECGimg_ts\"\n",
        "train_db=ECGimage_DB('train_image.csv',path)\n",
        "valid_db= ECGimage_DBv('val_image.csv',path)\n",
        "\n",
        "train_dl=DataLoader(train_db, batch_size=16)\n",
        "valid_dl=DataLoader(valid_db, batch_size=16)\n",
        "# image = read_image(os.path.join(path,'A0001.png'),mode=ImageReadMode.RGB)\n",
        "\n",
        "# print(image.size())\n",
        "# flatten = nn.Flatten()\n",
        "# flat_image = flatten(image).float()\n",
        "# print(flat_image.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TeA8awe-WbNN"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader,0):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Backpropagation\n",
        "        pred = model(X)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u2PLYn_CWfgN"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZmge5SXWjxJ",
        "outputId": "851ed083-07da-4b29-df8b-ee5a3ade14bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.608980  [    0/ 6189]\n",
            "loss: 1.874518  [ 1600/ 6189]\n",
            "loss: 1.910281  [ 3200/ 6189]\n",
            "loss: 2.052900  [ 4800/ 6189]\n",
            "Test Error: \n",
            " Accuracy: 36.5%, Avg loss: 1.774702 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.890113  [    0/ 6189]\n",
            "loss: 1.674151  [ 1600/ 6189]\n",
            "loss: 1.612850  [ 3200/ 6189]\n",
            "loss: 1.866413  [ 4800/ 6189]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.657797 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.731894  [    0/ 6189]\n",
            "loss: 1.505018  [ 1600/ 6189]\n",
            "loss: 1.409659  [ 3200/ 6189]\n",
            "loss: 1.701385  [ 4800/ 6189]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     train_loop(train_dl, model, loss_fn, optimizer)\n\u001b[0;32m      5\u001b[0m     test_loop(valid_dl, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn [9], line 4\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader,\u001b[39m0\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      5\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dl, model, loss_fn, optimizer)\n",
        "    test_loop(valid_dl, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5ehSdJlhbFfw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
